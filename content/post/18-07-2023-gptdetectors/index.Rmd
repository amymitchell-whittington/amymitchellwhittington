---
title: "GPT Detectors" 
author: "Amy Mitchell-Whittington"
date: 2023-07-18
categories: ["Tidy Tuesday"]
tags: ["TidyTuesday"]
image:
  caption: ''
  focal_point: ''
  preview_only: no
summary: Are GPT Detectors bias towards non-native English writers?
---

## GPT Detectors[</>](https://github.com/amymitchell-whittington/TidyTuesday/blob/main/2023/19-07-2023-gptdetectors/19-07-2023-gptdetectors.Rmd)  

The data this week comes from Simon Couch's [detectors R package](https://github.com/simonpcouch/detectors/), containing predictions from various GPT detectors. The data is based on the pre-print: [GPT Detectors Are Biased Against Non-Native English Writers](https://arxiv.org/abs/2304.02819). Weixin Liang, Mert Yuksekgonul, Yining Mao, Eric Wu, James Zou. 

language model-based chatbot ChatGPT is reportedly the fastest-growing consumer application in history, after attracting [100 million active users just two months after it was launched](https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/). It's easy to see why. The ability for such models to create large amounts of content within such a short period of time has opened the door wide open in terms of boosting productivity and creativity. From building cover letters for job applications to setting up company OKRs and everything in-between, people are using it in all kinds of ways. AI is even being used to detect AI-generated content, but how accurate has it been so far?

Liang et al. set out to test the accuracy of several GPT detectors:

> *The study authors carried out a series of experiments passing a number of essays to different GPT detection models.
> Juxtaposing detector predictions for papers written by native and non-native English writers, the authors argue that GPT
> detectors disproportionately classify real writing from non-native English writers as AI-generated.*

From the data, I was able to determine that GPT detectors correctly identified native English writers in 97% of cases, with only 3% being misclassified as AI-generated. However, the same was not true for non-native English writing samples, which were only correctly identified in 39% of cases. 

Interestingly, AI incorrectly identified AI-generated content as being created by a human in 69% of cases.

```{r set up, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 

library(ggplot2)
library(tidyverse)
library(tidyr)
library(dplyr)
library(readr)

library(waffle)

detectors <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-07-18/detectors.csv')
  
```

```{r explore, echo=FALSE, include = TRUE, message=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
pred_native <- detectors %>%
    summarize(n = n(), .by = c(".pred_class", "native")) %>%
  filter(!is.na(native), native == "Yes") %>% #NA under native column indicates copy was written by AI
  mutate(native_English_text = round(n/sum(n)*100)) %>%
  rename(predicted_class = ".pred_class") 

pred_nonnative <- detectors %>%
    summarize(n = n(), .by = c(".pred_class", "native")) %>%
  filter(!is.na(native), native == "No") %>%
  mutate(non.native_English_text = round(n/sum(n)*100)) %>%
    rename(predicted_class = ".pred_class")

pred_ai <- detectors %>%
    summarize(n = n(), .by = c(".pred_class", "native")) %>%
  filter(is.na(native)) %>%
  mutate(AI_text = round(n/sum(n)*100)) %>%
  rename(predicted_class = ".pred_class")

#combining each data frame into one in order to facet later on.

all_pred2 <- left_join(pred_native, pred_nonnative, by = "predicted_class") %>%
  left_join(pred_ai, by = "predicted_class") %>%
  select(predicted_class, native_English_text, non.native_English_text, AI_text)

  #only human samples

human_pred2 <- left_join(pred_native, pred_nonnative, by = "predicted_class") %>%
  select(predicted_class, native_English_text, non.native_English_text)

#to reformat data by changing from a "wide" format with each variable in its own column to a "long" format, to use one column for measures and another for the percent classification for each row.

gpt2datalong <- gather(all_pred2, key="measure", value="value", c("native_English_text", "non.native_English_text", "AI_text"))

gpt2datalong$measure <- factor(gpt2datalong$measure, levels=c("native_English_text", "non.native_English_text", "AI_text"))

  #same as above but with only human writing samples

human_predlong <- gather(human_pred2, key="measure", value="value", c("native_English_text", "non.native_English_text"))

human_predlong$measure <- factor(human_predlong$measure, levels=c("native_English_text", "non.native_English_text"))

#to change the title in the faceted graph:

human_predlong$measure <- factor(human_predlong$measure, levels = c("native_English_text", "non.native_English_text"), 
                  labels = c("native English samples", "non-native English samples"))

```

```{r table, echo = FALSE, results = 'asis'}
library(knitr)
kable(all_pred2)
```

I used a waffle chart to display the data because I think it's a great way to visualise the small number of categories and is easy to interpret and understand. This was my first waffle chart, so there was a lot of trial and error! I struggled to work out the correct variables to map at first (I kept getting a reps() error), but once that was sorted it was relatively smooth sailing.

I worked out how to reformat my data by changing from a "wide" format with each variable in its own column to a "long" format, so I could facet the data correctly. Then I changed the titles

I also discovered `theme(plot.margin = unit())`, which I used to change the margins of my plot.

```{r plot, echo=FALSE, message=FALSE, dpi = 300}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

#Human only

ggplot(human_predlong, aes(values = value, fill = predicted_class)) +
geom_waffle(rows = 5, na.rm = FALSE, show.legend = TRUE, flip = FALSE, colour = "white") +
  facet_wrap(~measure) +
   theme(panel.spacing.x = unit(0, "npc")) + #to create spacing around the title of each waffle chart.
  theme(strip.text.x = element_text(hjust = 0.5)) +
  coord_equal() +
  theme_void() +
  guides(fill = guide_legend(reverse=TRUE)) +
  scale_fill_manual(values = c("sienna3", "royalblue")) +
  labs(
    title = "Can GPT detectors tell what is written by humans vs. AI?",
    subtitle = "In a recent study, writing samples from native and non-native English writers were passed to GPT detectors,\nwhich misclassified 61% of non-native English writing samples as AI-generated, compared to just 3% of native English samples.\n  ",
    fill = "GPT Classification",
    caption = "Data source: GPT Detectors Are Biased Against Non-Native English Writers.\nWeixin Liang, Mert Yuksekgonul, Yining Mao, Eric Wu, James Zou.") +
  theme(
    plot.title = element_text(size = 10, face = "bold", hjust = 0),
    plot.subtitle = element_text(size = 7, face = "italic", hjust = 0),
    legend.title = element_text(size = 8, face = "bold"),
    plot.caption = element_text(size = 6),
    strip.text = element_text(size=7, face = "bold"),
    strip.clip = "off",
    plot.margin = unit(c(0.25, 0.5, 0.25, 0.5),
                                "inches"))
```

I originally opted to use the `iron()` function to knit all three waffle charts together, but it didn't give me as much freedom to tweak and alter the plot as `facet_wrap()`, so I switched it up. I also chose not to include the AI-generated content waffle chart, because it seemed to make the chart more complicated (with the headline/subline especially) than I wanted it to be. 

```{r alternate plots, message=FALSE, dpi = 300}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

#human and AI
ggplot(gpt2datalong, aes(values = value, fill = predicted_class)) +
geom_waffle(rows = 5, na.rm = FALSE, show.legend = TRUE, flip = TRUE, colour = "white") +
  facet_wrap(~measure) +
   theme(panel.spacing.x = unit(0, "npc")) +
  theme(strip.text.x = element_text(hjust = 0.5)) +
  coord_equal() +
  theme_void() +
  scale_fill_manual(values = c("sienna3", "royalblue")) +
  labs(
    title = "How accurate are GPT dectectors at correctly classifying human-written content vs. AI generated content?",
    subtitle = "",
    fill = "GPT Classification") +
  theme(
    plot.title = element_text(size = 8, hjust = 0),
    plot.subtitle = element_text(size = 5, face = "italic"),
    legend.title = element_text(size = 8))

```

```{r old plot data, include = FALSE, message=FALSE, dpi = 300}

#old plots using iron() function
plot_native <- ggplot(pred_native) +
geom_waffle(aes(values = native_English_text, fill = predicted_class), rows = 8, na.rm = FALSE, show.legend = TRUE, flip = TRUE, colour = "white") +
  coord_equal() +
  theme_void() +
  scale_fill_manual(values = c("sienna3", "royalblue")) +
  labs(
    tag = "How accurately do GPT detectors classify between\nnative English writers, non-native English writers, and AI?",
    title = "Native English author",
    subtitle = "xx") +
    #fill = "GPT Classification") +
  theme(
    plot.title = element_text(size = 8, hjust = 1),
    plot.subtitle = element_text(size = 5, face = "italic"),
    legend.title = element_text(size = 8))

plot_nonnative <- ggplot(pred_nonnative) +
geom_waffle(aes(values = non.native_English_text, fill = predicted_class), rows = 8, na.rm = FALSE, show.legend = TRUE, flip = TRUE, colour = "white") +
  theme_void() +
  scale_fill_manual(values = c("sienna3", "royalblue")) +
   labs(
    title = "Non-native English author",
        subtitle = "xx",
    fill = NULL) +
  theme(
    plot.title = element_text(size = 8),
    plot.subtitle = element_text(size = 5, face = "italic"))

plot_ai <- ggplot(pred_ai) +
geom_waffle(aes(values = AI_text, fill = predicted_class), rows = 8, na.rm = FALSE, show.legend = TRUE, flip = TRUE, colour = "white") +
  theme_void() +
  scale_fill_manual(values = c("sienna3", "royalblue")) +
   labs(
    title = "AI author",
    caption = "Data source: GPT Detectors Are Biased Against Non-Native English Writers.\nWeixin Liang, Mert Yuksekgonul, Yining Mao, Eric Wu, James Zou.",
        subtitle = "xx",
    fill = NULL) +
  theme(
    plot.title = element_text(size = 8),
    plot.subtitle = element_text(size = 5, face = "italic"),
    plot.caption = element_text(size = 5, face = "italic", hjust = 0))

iron(plot_native, plot_nonnative, plot_ai)
```